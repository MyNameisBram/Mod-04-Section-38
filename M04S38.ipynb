{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STUDY GROUP - M04S38\n",
    "## Big Data in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "You will be able to:\n",
    "* define \"Big Data\"\n",
    "* explain how MapReduce helps us deal with Big Data\n",
    "* understand SparkContext and RDD's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Data \n",
    "\n",
    "What are the 3 V's that define \"Big Data\"?\n",
    "\n",
    "**Volume:** primary attribute of big data. Quantified by size in the TBs and PBs [Byte types](https://www.redcentricplc.com/resources/infographics/byte-size/)\n",
    "\n",
    "**Velocity:** speed (frequency) at which data is generated \n",
    "- ex: everyday 900M photos are uploaded on Facebook, 500M tweets are posted, 0.4M hours of video are uploaded on Youtube and 3.5B searches on Google. \n",
    "\n",
    "**Variety:** structured or unstructed data. Most commonly added data are structered text, tweets, pictures & videos\n",
    "    \n",
    "What quantities of data are we talking about with \"Big Data\"?\n",
    "- Terabytes and Petabytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Reduce\n",
    "\n",
    "What is MapReduce used for?\n",
    "\n",
    "A: used to split big data, sets up in smaller sets to be distributed over several machines to deal with big data analytics. \n",
    "\n",
    "What are the steps in a MapReduce operation?\n",
    "\n",
    "- Map (Splitting & Mapping): \n",
    "    - takes one set of data and transform it into another set of data, where the individuals are broken down to tuples (key/value pairs).\n",
    "\n",
    "- Reduce (Shuffeling, Reducing): \n",
    "    - takes the output from a map as input and combines those data tuples into a smaller set of tuples. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Brief Description of each phase:**\n",
    "\n",
    "1. __Split__: Split the input data in a number of fragments according to size defined during configuration. (In hadoop file system \"HDFS\" the default block size is 64KB, whih can be changed if required). \n",
    "\n",
    "2. __Mapper__ : Each mapper processes a individual fragment rather than considering the whole document. A fragment only contains part of the document. In map phase we __tokenize__ words in each line of the given fragment as a  <Key,Value> pair. We set each token , we set `key = word`, and assign to this key  `value = 1`.\n",
    "\n",
    "3. __Shuffler__ : A shuffler (also called a __combiner__) sorts __all similar keys together__, to be passed to the reducer later.\n",
    "\n",
    "4. __Reducer__:  A Reducer process each sorted split. In words count example, reducer calculate the number of word occurrences based on the given value. \n",
    "\n",
    "We shall soon look at programming this simple example in an emulated SPARK environment. Meanwhile, [have at look at this article](https://www.edureka.co/blog/mapreduce-tutorial/) to see read about the in a bit more detail. In essence: \n",
    "\n",
    "> A MapReduce program is defined by at least two functions, creating 3 sets of key:value pairs:\n",
    "\n",
    "> map : **(k1, v1) −→ [(k2, v2)]**\n",
    "\n",
    "> reduce : **(k2, [v2]) −→ [(k3, v3)]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkContext:\n",
    "PySpark > Apache Spark (framework for big data processing) > "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkContext and RDD's\n",
    "\n",
    "What is a SparkContext? What does it do?\n",
    "- SparkContext is the entry point to any spark functionality\n",
    "- SparkContext is a client of Spark’s execution environment and it acts as the master of the Spark application.\n",
    "- SparkContext sets up internal services and establishes a connection to a Spark execution environment. \n",
    "- The driver is the program that creates the SparkContext, connecting to a given Spark Master. \n",
    "\n",
    "What is an RDD? (Resilient Distributed Datasets)\n",
    "- RDD is, essentially, the Spark representation of a set of data, spread across multiple machines, with APIs to let you act on it.\n",
    "- RDDs are not actual data, but they are Objects, which contains information about data residing on the cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "The key takeaways from this section include:\n",
    "\n",
    "* Big Data usually refers to datasets that grow so large that they become awkward to work with using traditional database management systems and analytical approaches\n",
    "* Big data refers to data that is terabytes (TB) to petabytes (PB) in size\n",
    "* Map-Reduce can be used to split big data sets up in smaller sets to be distributed over several machines to deal with Big Data Analytics.\n",
    "* Before starting to work, you need to install Docker and Kinematic on your environment\n",
    "* Make sure to test your installation so you're sure everything is working\n",
    "* When you start working with PySpark, you have to create a `SparkContext()`.\n",
    "* The creation or RDDs is essential when working with PySpark\n",
    "* Examples of actions and transformations include `collect()`, `count()`, `filter()`, `first()`, `take()`, and `reduce()`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
